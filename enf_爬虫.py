import datetime
import re
import time

import requests
from bs4 import BeautifulSoup
import urllib.request
import csv
from lxml import etree
import os

def log(str):
    print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') + " " + str)


log('é‡‡é›†ç³»ç»Ÿå¯åŠ¨æˆåŠŸâœ¨ğŸŒˆğŸ˜€...')
urlpage = input('è¯·è¾“å…¥ä½ è¦é‡‡é›†çš„ç½‘ç«™é“¾æ¥ï¼š')
# æŠŠç½‘å€ URL å­˜åœ¨å˜é‡é‡Œ
# urlpage = 'https://www.enf.com.cn/directory/software'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/80.0.3987.149 Safari/537.36 '
}

page = urllib.request.urlopen(urlpage)
soup = BeautifulSoup(page, 'html.parser')
title = soup.title.text
title = ''.join(title.split())
log('é¡µé¢åŠ è½½å®Œæˆï¼Œç½‘ç«™æ ‡é¢˜æ˜¯ï¼š'+title)
with open(os.path.abspath("çˆ¬è™«é‡‡é›†åˆ°çš„æ•°æ®è¡¨.csv"), "w", newline='') as f:
    writer = csv.writer(f)
    # å…ˆå†™å…¥columns_name
    writer.writerow(['company', 'address', 'website', 'telephone', 'email'])
pagination = soup.find('ul', attrs={'class': 'enf-pagination'})
paginations = len(pagination.find_all('li')) - 1
log('ç³»ç»Ÿæ£€æµ‹åˆ°ä¸€å…±æœ‰' + str(paginations) + 'é¡µæ•°æ®')
for pagination in range(paginations):
    time.sleep(2)
    pagination = str(pagination + 1)
    log('å¼€å§‹åŠ è½½ç¬¬'+ pagination +'é¡µæ•°æ®')
    page = urllib.request.urlopen(urlpage+'?page='+pagination)
    soup = BeautifulSoup(page, 'html.parser')
    # åœ¨è¡¨æ ¼ä¸­æŸ¥æ‰¾æ•°æ®
    table = soup.find('table', attrs={'class': 'enf-list-table'})
    results = table.find_all('tr')
    links = []
    companies = []
    log('åŠ è½½æˆåŠŸï¼å½“å‰é¡µé¢æ£€æµ‹åˆ°éœ€è¦é‡‡é›†çš„å…¬å¸æ•°é‡æœ‰ï¼š' + str(len(results)))
    # éå†æ‰€æœ‰æ•°æ®
    for index in range(len(results)):
        result = results[index]
        # æ‰¾åˆ°æ¯ä¸€ä¸ª td å•å…ƒæ ¼çš„å†…å®¹
        data = result.find('td').find('a')
        link = str(data.get('href')).strip()
        company = str(data.getText()).strip()
        links.append(link)
        companies.append(company)
        # å¦‚æœè¯¥å•å…ƒæ ¼æ— æ•°æ®ï¼Œåˆ™è·³è¿‡
        if len(data) == 0:
            continue


    for index, link in enumerate(links):
        company = companies[index]
        log('å¼€å§‹é‡‡é›†ç¬¬'+ pagination +'é¡µç¬¬'+ str(index +1) + 'æ¡æ•°æ®.å…¬å¸åç§°ï¼š'+company)
        # è·å–ç½‘é¡µå†…å®¹ï¼ŒæŠŠ HTML æ•°æ®ä¿å­˜åœ¨ page å˜é‡ä¸­
        page = urllib.request.urlopen(link)
        # ç”¨ Beautiful Soup è§£æ html æ•°æ®ï¼Œ
        # å¹¶ä¿å­˜åœ¨ soup å˜é‡é‡Œ
        soup = BeautifulSoup(page, 'html.parser')
        # åœ¨è¡¨æ ¼ä¸­æŸ¥æ‰¾æ•°æ®
        try:
            table = soup.find('div', attrs={'class': 'enf-company-profile-info-main-spec'})
            address = table.find('td', attrs={'itemprop': 'address'}).getText().strip()
            website = table.find('a', attrs={'itemprop': 'url'}).getText().strip()
            telephone = table.find('td', attrs={'itemprop': 'telephone'})
        except:
            log('ã€è­¦å‘Šâš ã€‘æ— æ³•è¯†åˆ«çš„æ ¼å¼ï¼Œå·²å¿½ç•¥' + company)
            continue

        try:
            telephoneSpan = telephone.find('span')
            email = table.find('td', attrs={'itemprop': 'email'})
            emailSpan = email.find('span')
        except:
            email = 'null'

        if telephoneSpan is not None:
            pattern = re.compile(r"\'(.*?)\'")
            token = pattern.findall(str(telephoneSpan))
            api = "https://www.enf.com.cn/api/company-phone/" + token[0]
            telephone = requests.get(api, headers=headers).text
            time.sleep(1)
        else:
            telephone = telephone.getText().strip()

        if emailSpan is not None:
            pattern = re.compile(r"\'(.*?)\'")
            token = pattern.findall(str(emailSpan))
            api = "https://www.enf.com.cn/company_email/" + token[0]
            email = requests.get(api, headers=headers).text
            time.sleep(1)
        else:
            try:
                email.getText().strip()
            except:
                email = 'null'

        if telephone == 'å·²è¾¾åˆ°æ¯æ—¥è¯·æ±‚é™åˆ¶' or email == 'å·²è¾¾åˆ°æ¯æ—¥è¯·æ±‚é™åˆ¶':
            log('ã€è­¦å‘Šâš ã€‘æ‚¨çš„IPåœ°å€å·²è¾¾åˆ°æ¯æ—¥è¯·æ±‚é™åˆ¶ï¼Œè¯·æ‰‹åŠ¨åˆ‡æ¢IPåæŒ‰ä»»æ„é”®ç»§ç»­é‡‡é›†')
            input()

        with open("çˆ¬è™«é‡‡é›†åˆ°çš„æ•°æ®è¡¨.csv", "a", newline='') as f:
            writer = csv.writer(f)
            writer.writerow([company, address, website, telephone, email])
        log('ç¬¬' + str(index + 1) + 'æ¡æ•°æ®é‡‡é›†å®Œæ¯•ï¼Œä¿å­˜æˆåŠŸï¼âœ¨ğŸŒˆâœ¨ğŸŒˆ')
log('æ•°æ®é‡‡é›†å®Œæ¯•ï¼Œæ„Ÿè°¢æ‚¨çš„ä½¿ç”¨ï¼')
